---
layout: default
title: SWE-Bench Progression Timeline
description: Interactive visualization of AI model performance on coding benchmarks
---
    <header>
        <h1>SWE-Bench Progression Timeline</h1>
        <p>Historical progression of AI model performance on the SWE-Bench coding benchmark</p>
    </header>

    <div class="methodology-callout">
        <h3>📋 Methodology & Analysis</h3>
        <p><strong>Note:</strong> These are preliminary results based on available data. Please consult both methodology and critical analysis documents:</p>
        <div class="methodology-links">
            <a href="PROJECTION_METHODOLOGY" class="method-link">📊 Projection Methodology</a>
            <a href="METHODOLOGY_VALIDITY_ANALYSIS" class="method-link">🔍 Critical Validity Analysis</a>
        </div>
        <p class="disclaimer">Projections are speculative and subject to significant uncertainty. External validation recommended.</p>
    </div>

    <main>
        <section class="chart-container">
            <div class="chart-wrapper">
                <canvas id="timeline-chart"></canvas>
            </div>
        </section>
        
        <section class="insights">
            <div class="insights-grid">
                <div class="insight-card">
                    <h3>🚀 Exponential Progress</h3>
                    <p>AI coding performance jumped from 2% (Claude 2, Nov 2023) to 75% (Claude 4.1 Opus, Aug 2025) in just 21 months - a 37x improvement.</p>
                </div>
                
                <div class="insight-card">
                    <h3>👨‍💻 Human Parity Achieved</h3>
                    <p>Current top models (Claude Opus 75%, GPT-5 65%) have surpassed professional human performance (70%) and are approaching expert levels (85%).</p>
                </div>
                
                <div class="insight-card">
                    <h3>💪 Compute Scaling Laws</h3>
                    <p>Strong correlation between FLOPS capacity and performance. Nvidia's roadmap (60x compute increase by 2027) suggests continued rapid progress.</p>
                </div>
                
                <div class="insight-card">
                    <h3>🧠 Context is King</h3>
                    <p>Context length scaling from 8K to 50M tokens (6000x increase) enables handling entire codebases, documentation, and complex multi-file tasks.</p>
                </div>
                
                <div class="insight-card">
                    <h3>🎯 Near-Perfect by 2027</h3>
                    <p>Claude Opus projection suggests 98% performance by mid-2027, approaching the theoretical 100% ceiling on SWE-bench tasks.</p>
                </div>
                
                <div class="insight-card">
                    <h3>⚡ Three-Horse Race</h3>
                    <p>Anthropic (Claude) leads with 75%, OpenAI (GPT) reaches 65%, Google (Gemini Pro) at 54%. All three are accelerating rapidly with different architectural approaches.</p>
                </div>
                
                <div class="insight-card">
                    <h3>📊 Projection Methodology</h3>
                    <p>Claude Opus projections combine dual scaling factors: 60% weight on FLOPS capacity (Nvidia roadmap) + 40% on context length growth, using exponential curves with diminishing returns toward 100% ceiling.</p>
                </div>
                
                <div class="insight-card">
                    <h3>🎚️ Why Not 100%?</h3>
                    <p>Even 98% projection reflects realistic constraints: edge cases requiring human judgment, multi-step reasoning limits, ambiguous requirements, and the inherent complexity of some real-world software engineering tasks.</p>
                </div>
            </div>
        </section>
    </main>

    <script src="{{ "/script.js" | relative_url }}"></script>